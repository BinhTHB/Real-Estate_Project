#!/usr/bin/env python3
"""
PostgreSQL Analytics Runner - Ch·∫°y analytics queries tr√™n d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n
"""

import os
import sys
import logging
import pandas as pd
import duckdb
import boto3
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError
import yaml
from typing import Optional, List, Dict, Any
from sql_query_generator import RealEstateSQLGenerator
import argparse

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MinioToPostgresExporter:
    """Class ƒë·ªÉ ch·∫°y analytics queries tr√™n d·ªØ li·ªáu PostgreSQL"""

    def __init__(self, postgres_url: str, minio_config: Dict[str, Any]):
        """
        Kh·ªüi t·∫°o exporter

        Args:
            postgres_url: PostgreSQL connection URL
            minio_config: C·∫•u h√¨nh MinIO (endpoint, credentials, etc.)
        """
        self.postgres_url = postgres_url
        self.minio_config = minio_config
        self.engine = None
        self.duckdb_conn = None

    def connect_postgres(self):
        """K·∫øt n·ªëi t·ªõi PostgreSQL"""
        try:
            self.engine = create_engine(
                self.postgres_url,
                pool_size=10,
                max_overflow=20,
                pool_timeout=30,
                pool_recycle=3600
            )
            # Test connection
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            logger.info("‚úÖ K·∫øt n·ªëi PostgreSQL th√†nh c√¥ng")
        except SQLAlchemyError as e:
            logger.error(f"‚ùå L·ªói k·∫øt n·ªëi PostgreSQL: {e}")
            raise

    def connect_minio_duckdb(self):
        """K·∫øt n·ªëi t·ªõi MinIO qua DuckDB"""
        try:
            # C·∫•u h√¨nh DuckDB ƒë·ªÉ k·∫øt n·ªëi MinIO S3
            self.duckdb_conn = duckdb.connect()
            self.duckdb_conn.execute(f"""
                INSTALL httpfs;
                LOAD httpfs;
                SET s3_endpoint='{self.minio_config['endpoint']}';
                SET s3_access_key_id='{self.minio_config['access_key']}';
                SET s3_secret_access_key='{self.minio_config['secret_key']}';
                SET s3_use_ssl=false;
                SET s3_url_style='path';
            """)
            logger.info("‚úÖ K·∫øt n·ªëi MinIO qua DuckDB th√†nh c√¥ng")
        except Exception as e:
            logger.error(f"‚ùå L·ªói k·∫øt n·ªëi MinIO: {e}")
            raise

    def create_table_from_parquet(self, file_url: str, table_name: str, batch_size: int = 1000):
        """
        T·∫°o b·∫£ng PostgreSQL t·ª´ file Parquet v√† insert d·ªØ li·ªáu

        Args:
            file_url: URL c·ªßa file Parquet trong MinIO
            table_name: T√™n b·∫£ng PostgreSQL
            batch_size: K√≠ch th∆∞·ªõc batch cho insert
        """
        try:
            # Convert s3:// URL to http:// URL for DuckDB
            if file_url.startswith('s3://'):
                # s3://bucket/key -> http://endpoint/bucket/key
                bucket_and_key = file_url[5:]  # Remove 's3://'
                http_url = f"{self.minio_config['endpoint']}/{bucket_and_key}"
            else:
                http_url = file_url

            logger.info(f"üìÑ ƒê·ªçc file t·ª´: {http_url}")

            # ƒê·ªçc sample ƒë·ªÉ x√°c ƒë·ªãnh schema
            df_sample = self.duckdb_conn.sql(f"SELECT * FROM read_parquet('{http_url}') LIMIT 1").df()

            if df_sample.empty:
                logger.warning(f"‚ö†Ô∏è File {http_url} tr·ªëng, b·ªè qua")
                return

            # Mapping ki·ªÉu d·ªØ li·ªáu t·ª´ pandas sang PostgreSQL
            type_mapping = {
                'object': 'TEXT',
                'int64': 'BIGINT',
                'float64': 'DOUBLE PRECISION',
                'bool': 'BOOLEAN',
                'datetime64[ns]': 'TIMESTAMP'
            }

            # Sanitize column names for PostgreSQL
            import unicodedata
            import re
            
            def sanitize_column_name(name):
                # Remove accents and special characters
                name = unicodedata.normalize('NFD', str(name))
                name = name.encode('ascii', 'ignore').decode('ascii')
                # Replace spaces and special chars with underscore
                name = re.sub(r'[^a-zA-Z0-9]', '_', name)
                # Remove multiple underscores
                name = re.sub(r'_+', '_', name)
                # Remove leading/trailing underscores
                name = name.strip('_')
                # Ensure not empty
                if not name:
                    name = 'column'
                return name.lower()

            # T·∫°o c√¢u l·ªánh CREATE TABLE
            columns = []
            column_mapping = {}  # Map original name to sanitized name
            for col, dtype in df_sample.dtypes.items():
                sanitized_col = sanitize_column_name(col)
                pg_type = type_mapping.get(str(dtype), 'TEXT')
                columns.append(f'"{sanitized_col}" {pg_type}')
                column_mapping[col] = sanitized_col

            create_table_sql = f"""
            DROP TABLE IF EXISTS {table_name};
            CREATE TABLE {table_name} (
                {', '.join(columns)}
            );
            """

            # Th·ª±c thi CREATE TABLE
            with self.engine.connect() as conn:
                conn.execute(text(create_table_sql))
                conn.commit()

            logger.info(f"‚úÖ ƒê√£ t·∫°o b·∫£ng {table_name}")

            # Insert d·ªØ li·ªáu theo batch
            offset = 0
            while True:
                batch_df = self.duckdb_conn.sql(f"SELECT * FROM read_parquet('{http_url}') LIMIT {batch_size} OFFSET {offset}").df()
                if batch_df.empty:
                    break

                # Rename columns to sanitized names
                batch_df_renamed = batch_df.rename(columns=column_mapping)

                # Insert batch
                batch_df_renamed.to_sql(table_name, self.engine, if_exists='append', index=False)
                offset += batch_size
                logger.info(f"üì• ƒê√£ insert {len(batch_df)} d√≤ng (offset: {offset})")

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o b·∫£ng t·ª´ {file_url}: {e}")
            raise

    def get_parquet_files(self, bucket_name: str, prefix: str) -> List[str]:
        """L·∫•y danh s√°ch file Parquet t·ª´ MinIO s·ª≠ d·ª•ng boto3"""
        try:
            # S·ª≠ d·ª•ng boto3 ƒë·ªÉ k·∫øt n·ªëi MinIO
            s3_client = boto3.client(
                's3',
                endpoint_url=self.minio_config['endpoint'],
                aws_access_key_id=self.minio_config['access_key'],
                aws_secret_access_key=self.minio_config['secret_key'],
                region_name=self.minio_config.get('region', 'us-east-1')
            )

            # List objects v·ªõi prefix
            response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
            
            files = []
            if 'Contents' in response:
                for obj in response['Contents']:
                    key = obj['Key']
                    if key.endswith('.parquet'):
                        files.append(f"s3://{bucket_name}/{key}")
            
            logger.info(f"üìÅ T√¨m th·∫•y {len(files)} file Parquet")
            for file in files[:5]:  # Log first 5 files
                logger.info(f"  - {file}")
            return files
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l·∫•y danh s√°ch file v·ªõi boto3: {e}")
            return []

    def export_data(self, parquet_files: List[str], table_name: str):
        """Export t·∫•t c·∫£ d·ªØ li·ªáu t·ª´ Parquet files sang PostgreSQL"""
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu export {len(parquet_files)} file Parquet sang {table_name}")

        for i, file_url in enumerate(parquet_files, 1):
            logger.info(f"üìÑ ƒêang x·ª≠ l√Ω file {i}/{len(parquet_files)}: {file_url}")
            self.create_table_from_parquet(file_url, table_name)

        logger.info("‚úÖ Ho√†n th√†nh export d·ªØ li·ªáu")

    def create_indexes(self, table_name: str):
        """T·∫°o indexes cho b·∫£ng ƒë·ªÉ t·ªëi ∆∞u performance"""
        # Ch·ªâ t·∫°o index cho c√°c c·ªôt ch·∫Øc ch·∫Øn t·ªìn t·∫°i
        indexes = [
            f"CREATE INDEX IF NOT EXISTS idx_{table_name}_ia_chi ON {table_name}(ia_chi);",
            f"CREATE INDEX IF NOT EXISTS idx_{table_name}_url ON {table_name}(url);",
            f"CREATE INDEX IF NOT EXISTS idx_{table_name}_latitude ON {table_name}(latitude);",
            f"CREATE INDEX IF NOT EXISTS idx_{table_name}_longitude ON {table_name}(longitude);"
        ]

        try:
            with self.engine.connect() as conn:
                for index_sql in indexes:
                    conn.execute(text(index_sql))
                    conn.commit()
            logger.info("‚úÖ ƒê√£ t·∫°o indexes")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o indexes: {e}")

    def verify_export(self, table_name: str, expected_count: int):
        """Verify s·ªë l∆∞·ª£ng d·ªØ li·ªáu ƒë√£ export"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                actual_count = result.fetchone()[0]

            logger.info(f"üìä S·ªë d√≤ng trong PostgreSQL: {actual_count}")
            logger.info(f"üìä S·ªë d√≤ng expected: {expected_count}")

            if actual_count == expected_count:
                logger.info("‚úÖ S·ªë l∆∞·ª£ng d·ªØ li·ªáu kh·ªõp!")
            else:
                logger.warning(f"‚ö†Ô∏è S·ªë l∆∞·ª£ng kh√¥ng kh·ªõp: {actual_count} vs {expected_count}")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi verify: {e}")

    def generate_and_run_queries(self, table_name: str, run_queries: bool = True, limit_results: int = 10) -> Dict[str, Any]:
        """Generate v√† ch·∫°y c√°c SQL queries analytics"""
        sql_gen = RealEstateSQLGenerator()
        results = {}

        queries = {
            'basic_stats': sql_gen.generate_basic_stats_query(table_name),
            'price_by_city': sql_gen.generate_price_by_city_query(table_name),
            'location_distribution': sql_gen.generate_property_type_distribution_query(table_name),
            'price_ranges': sql_gen.generate_price_ranges_query(table_name)
        }

        if run_queries:
            try:
                for query_name, query_sql in queries.items():
                    logger.info(f"üîç Ch·∫°y query: {query_name}")
                    with self.engine.connect() as conn:
                        result = conn.execute(text(query_sql))
                        df = pd.DataFrame(result.fetchall(), columns=result.keys())
                        results[query_name] = df.head(limit_results)
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi ch·∫°y queries: {e}")

        return results

    def print_query_results(self, results: Dict[str, Any]):
        """In k·∫øt qu·∫£ queries"""
        for query_name, df in results.items():
            print(f"\n=== {query_name.upper()} ===")
            print(df.to_string(index=False))

def main():
    parser = argparse.ArgumentParser(description='Ch·∫°y analytics queries tr√™n d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n trong PostgreSQL')
    parser.add_argument('--limit-results', type=int, default=10, help='Gi·ªõi h·∫°n s·ªë d√≤ng k·∫øt qu·∫£ queries')

    args = parser.parse_args()

    # C·∫•u h√¨nh PostgreSQL m·∫∑c ƒë·ªãnh
    POSTGRES_CONFIG = {}

    # T√¨m file credentials
    possible_paths = [
        'postgres_credentials.yaml',
        os.path.join(os.path.dirname(__file__), 'postgres_credentials.yaml')
    ]

    creds_file = None
    for path in possible_paths:
        if os.path.exists(path):
            creds_file = path
            break

    if creds_file:
        try:
            with open(creds_file, 'r', encoding='utf-8') as f:
                creds = yaml.safe_load(f)
            POSTGRES_CONFIG['host'] = creds.get('postgresql', {}).get('host')
            POSTGRES_CONFIG['port'] = creds.get('postgresql', {}).get('port')
            POSTGRES_CONFIG['database'] = creds.get('postgresql', {}).get('database')
            POSTGRES_CONFIG['user'] = creds.get('postgresql', {}).get('user')
            POSTGRES_CONFIG['password'] = creds.get('postgresql', {}).get('password')
            logger.info(f"‚úÖ ƒê√£ t·∫£i credentials t·ª´: {creds_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói khi t·∫£i credentials: {e}")

    # Fallback defaults
    if not POSTGRES_CONFIG.get('host'):
        POSTGRES_CONFIG['host'] = 'localhost'
    if not POSTGRES_CONFIG.get('port'):
        POSTGRES_CONFIG['port'] = 5432
    if not POSTGRES_CONFIG.get('database'):
        POSTGRES_CONFIG['database'] = 'real_estate_db'
    if not POSTGRES_CONFIG.get('user'):
        POSTGRES_CONFIG['user'] = 'postgres'
    if not POSTGRES_CONFIG.get('password'):
        POSTGRES_CONFIG['password'] = 'postgres123'

    # C·∫•u h√¨nh MinIO
    MINIO_CONFIG = {
        'endpoint': 'http://127.0.0.1:9000',
        'access_key': 'minioadmin',
        'secret_key': 'minioadmin',
        'region': 'us-east-1',
        'use_ssl': False
    }

    # T·∫°o PostgreSQL URL
    postgres_url = f"postgresql://{POSTGRES_CONFIG['user']}:{POSTGRES_CONFIG['password']}@{POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}/{POSTGRES_CONFIG['database']}"

    # Kh·ªüi t·∫°o exporter
    exporter = MinioToPostgresExporter(postgres_url, MINIO_CONFIG)

    try:
        # K·∫øt n·ªëi PostgreSQL
        exporter.connect_postgres()

        # Ch·ªâ ch·∫°y queries tr√™n d·ªØ li·ªáu ƒë√£ c√≥
        table_name = "real_estate_properties"
        logger.info("üîç Generating v√† ch·∫°y c√°c c√¢u l·ªánh SQL truy v·∫•n...")
        query_results = exporter.generate_and_run_queries(
            table_name=table_name,
            run_queries=True,
            limit_results=args.limit_results
        )
        exporter.print_query_results(query_results)

        logger.info("üéâ Ho√†n th√†nh generate queries!")

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh ch·∫°y queries: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()