#!/usr/bin/env python3
"""
PostgreSQL Analytics Runner - Ch·∫°y analytics queries tr√™n d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n
"""

import os
import sys
import logging
import pandas as pd
import duckdb
import boto3
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError
import yaml
from typing import Optional, List, Dict, Any
from sql_query_generator import RealEstateSQLGenerator
import argparse

# Th√™m import cho plotting
try:
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')  # Use non-interactive backend for server environments
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False
    print("‚ö†Ô∏è Matplotlib kh√¥ng kh·∫£ d·ª•ng, b·ªè qua t√≠nh nƒÉng v·∫Ω bi·ªÉu ƒë·ªì")

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MinioToPostgresExporter:
    """Class ƒë·ªÉ ch·∫°y analytics queries tr√™n d·ªØ li·ªáu PostgreSQL"""

    def __init__(self, postgres_url: str, minio_config: Dict[str, Any]):
        """
        Kh·ªüi t·∫°o exporter

        Args:
            postgres_url: PostgreSQL connection URL
            minio_config: C·∫•u h√¨nh MinIO (endpoint, credentials, etc.)
        """
        self.postgres_url = postgres_url
        self.minio_config = minio_config
        self.engine = None
        self.duckdb_conn = None

    def connect_postgres(self):
        """K·∫øt n·ªëi t·ªõi PostgreSQL"""
        try:
            self.engine = create_engine(
                self.postgres_url,
                pool_size=10,
                max_overflow=20,
                pool_timeout=30,
                pool_recycle=3600
            )
            # Test connection
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            logger.info(" K·∫øt n·ªëi PostgreSQL th√†nh c√¥ng")
        except SQLAlchemyError as e:
            logger.error(f"‚ùå L·ªói k·∫øt n·ªëi PostgreSQL: {e}")
            raise

    def connect_minio_duckdb(self):
        """K·∫øt n·ªëi t·ªõi MinIO qua DuckDB"""
        try:
            # C·∫•u h√¨nh DuckDB ƒë·ªÉ k·∫øt n·ªëi MinIO S3
            self.duckdb_conn = duckdb.connect()
            self.duckdb_conn.execute(f"""
                INSTALL httpfs;
                LOAD httpfs;
                SET s3_endpoint='{self.minio_config['endpoint']}';
                SET s3_access_key_id='{self.minio_config['access_key']}';
                SET s3_secret_access_key='{self.minio_config['secret_key']}';
                SET s3_use_ssl=false;
                SET s3_url_style='path';
            """)
            logger.info("‚úÖ K·∫øt n·ªëi MinIO qua DuckDB th√†nh c√¥ng")
        except Exception as e:
            logger.error(f"‚ùå L·ªói k·∫øt n·ªëi MinIO: {e}")
            raise

    def create_table_from_parquet(self, file_url: str, table_name: str, batch_size: int = 1000, create_table_only: bool = False):
        """
        T·∫°o b·∫£ng PostgreSQL t·ª´ file Parquet v√† insert d·ªØ li·ªáu

        Args:
            file_url: URL c·ªßa file Parquet trong MinIO
            table_name: T√™n b·∫£ng PostgreSQL
            batch_size: K√≠ch th∆∞·ªõc batch cho insert
            create_table_only: Ch·ªâ t·∫°o b·∫£ng, kh√¥ng insert d·ªØ li·ªáu
        """
        try:
            # Convert s3:// URL to http:// URL for DuckDB
            if file_url.startswith('s3://'):
                # s3://bucket/key -> http://endpoint/bucket/key
                bucket_and_key = file_url[5:]  # Remove 's3://'
                http_url = f"{self.minio_config['endpoint']}/{bucket_and_key}"
            else:
                http_url = file_url

            logger.info(f"üìÑ ƒê·ªçc file t·ª´: {http_url}")

            # ƒê·ªçc sample ƒë·ªÉ x√°c ƒë·ªãnh schema
            df_sample = self.duckdb_conn.sql(f"SELECT * FROM read_parquet('{http_url}') LIMIT 1").df()

            if df_sample.empty:
                logger.warning(f"‚ö†Ô∏è File {http_url} tr·ªëng, b·ªè qua")
                return

            # Mapping ki·ªÉu d·ªØ li·ªáu t·ª´ pandas sang PostgreSQL
            type_mapping = {
                'object': 'TEXT',
                'int64': 'BIGINT',
                'float64': 'DOUBLE PRECISION',
                'bool': 'BOOLEAN',
                'datetime64[ns]': 'TIMESTAMP'
            }

            # Sanitize column names for PostgreSQL
            import unicodedata
            import re
            
            def sanitize_column_name(name):
                # Remove accents and special characters
                name = unicodedata.normalize('NFD', str(name))
                name = name.encode('ascii', 'ignore').decode('ascii')
                # Replace spaces and special chars with underscore
                name = re.sub(r'[^a-zA-Z0-9]', '_', name)
                # Remove multiple underscores
                name = re.sub(r'_+', '_', name)
                # Remove leading/trailing underscores
                name = name.strip('_')
                # Ensure not empty
                if not name:
                    name = 'column'
                return name.lower()

            # T·∫°o c√¢u l·ªánh CREATE TABLE
            columns = []
            column_mapping = {}  # Map original name to sanitized name
            for col, dtype in df_sample.dtypes.items():
                sanitized_col = sanitize_column_name(col)
                pg_type = type_mapping.get(str(dtype), 'TEXT')
                columns.append(f'"{sanitized_col}" {pg_type}')
                column_mapping[col] = sanitized_col

            create_table_sql = f"""
            DROP TABLE IF EXISTS {table_name};
            CREATE TABLE {table_name} (
                {', '.join(columns)}
            );
            """

            # Th·ª±c thi CREATE TABLE
            with self.engine.connect() as conn:
                conn.execute(text(create_table_sql))
                conn.commit()

            logger.info(f"‚úÖ ƒê√£ t·∫°o b·∫£ng {table_name}")

            # Ch·ªâ insert d·ªØ li·ªáu n·∫øu kh√¥ng ph·∫£i ch·∫ø ƒë·ªô create_table_only
            if not create_table_only:
                # Insert d·ªØ li·ªáu theo batch
                offset = 0
                while True:
                    batch_df = self.duckdb_conn.sql(f"SELECT * FROM read_parquet('{http_url}') LIMIT {batch_size} OFFSET {offset}").df()
                    if batch_df.empty:
                        break

                    # Rename columns to sanitized names
                    batch_df_renamed = batch_df.rename(columns=column_mapping)

                    # Insert batch
                    batch_df_renamed.to_sql(table_name, self.engine, if_exists='append', index=False)
                    offset += batch_size
                    logger.info(f"üì• ƒê√£ insert {len(batch_df)} d√≤ng (offset: {offset})")

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o b·∫£ng t·ª´ {file_url}: {e}")
            raise

    def insert_data_from_parquet(self, file_url: str, table_name: str, batch_size: int = 1000):
        """
        Insert d·ªØ li·ªáu t·ª´ file Parquet v√†o b·∫£ng PostgreSQL ƒë√£ t·ªìn t·∫°i

        Args:
            file_url: URL c·ªßa file Parquet trong MinIO
            table_name: T√™n b·∫£ng PostgreSQL
            batch_size: K√≠ch th∆∞·ªõc batch cho insert
        """
        try:
            # Convert s3:// URL to http:// URL for DuckDB
            if file_url.startswith('s3://'):
                bucket_and_key = file_url[5:]  # Remove 's3://'
                http_url = f"{self.minio_config['endpoint']}/{bucket_and_key}"
            else:
                http_url = file_url

            logger.info(f"üìÑ Insert d·ªØ li·ªáu t·ª´: {http_url}")

            # ƒê·ªçc sample ƒë·ªÉ l·∫•y column mapping (gi·∫£ s·ª≠ b·∫£ng ƒë√£ ƒë∆∞·ª£c t·∫°o v·ªõi c√πng schema)
            df_sample = self.duckdb_conn.sql(f"SELECT * FROM read_parquet('{http_url}') LIMIT 1").df()

            if df_sample.empty:
                logger.warning(f"‚ö†Ô∏è File {http_url} tr·ªëng, b·ªè qua")
                return

            # Sanitize column names (gi·ªëng nh∆∞ trong create_table_from_parquet)
            import unicodedata
            import re

            def sanitize_column_name(name):
                name = unicodedata.normalize('NFD', str(name))
                name = name.encode('ascii', 'ignore').decode('ascii')
                name = re.sub(r'[^a-zA-Z0-9]', '_', name)
                name = re.sub(r'_+', '_', name)
                name = name.strip('_')
                if not name:
                    name = 'column'
                return name.lower()

            # T·∫°o column mapping
            column_mapping = {}
            for col in df_sample.columns:
                column_mapping[col] = sanitize_column_name(col)

            # Insert d·ªØ li·ªáu theo batch
            offset = 0
            total_inserted = 0
            while True:
                batch_df = self.duckdb_conn.sql(f"SELECT * FROM read_parquet('{http_url}') LIMIT {batch_size} OFFSET {offset}").df()
                if batch_df.empty:
                    break

                # Rename columns to sanitized names
                batch_df_renamed = batch_df.rename(columns=column_mapping)

                # Insert batch
                batch_df_renamed.to_sql(table_name, self.engine, if_exists='append', index=False)
                batch_count = len(batch_df)
                total_inserted += batch_count
                offset += batch_size
                logger.info(f"üì• ƒê√£ insert {batch_count} d√≤ng (t·ªïng: {total_inserted}, offset: {offset})")

            logger.info(f"‚úÖ Ho√†n th√†nh insert {total_inserted} d√≤ng t·ª´ {file_url}")

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi insert d·ªØ li·ªáu t·ª´ {file_url}: {e}")
            raise

    def read_and_deduplicate_delta_data(self, parquet_files: List[str]) -> pd.DataFrame:
        """
        ƒê·ªçc t·∫•t c·∫£ d·ªØ li·ªáu t·ª´ Delta Lake v√† lo·∫°i b·ªè duplicate d·ª±a tr√™n propertydetails_propertyid

        Args:
            parquet_files: Danh s√°ch file Parquet

        Returns:
            DataFrame ƒë√£ deduplicate
        """
        try:
            all_dataframes = []

            for file_url in parquet_files:
                # Convert s3:// URL to http:// URL for DuckDB
                if file_url.startswith('s3://'):
                    bucket_and_key = file_url[5:]  # Remove 's3://'
                    http_url = f"{self.minio_config['endpoint']}/{bucket_and_key}"
                else:
                    http_url = file_url

                logger.info(f"üìÑ ƒê·ªçc file: {http_url}")

                # ƒê·ªçc to√†n b·ªô file
                df = self.duckdb_conn.sql(f"SELECT * FROM read_parquet('{http_url}')").df()
                if not df.empty:
                    all_dataframes.append(df)

            if not all_dataframes:
                logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu t·ª´ c√°c file Parquet")
                return pd.DataFrame()

            # G·ªôp t·∫•t c·∫£ DataFrames
            combined_df = pd.concat(all_dataframes, ignore_index=True)

            # ƒê·∫øm s·ªë b·∫£n ghi tr∆∞·ªõc deduplicate
            total_before = len(combined_df)
            logger.info(f"üìä T·ªïng s·ªë b·∫£n ghi tr∆∞·ªõc deduplicate: {total_before}")

            # Deduplicate d·ª±a tr√™n propertydetails_propertyid (n·∫øu c√≥) ho·∫∑c url
            if 'propertydetails_propertyid' in combined_df.columns:
                dedup_column = 'propertydetails_propertyid'
            elif 'url' in combined_df.columns:
                dedup_column = 'url'
            else:
                logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt ƒë·ªÉ deduplicate, gi·ªØ nguy√™n d·ªØ li·ªáu")
                return combined_df

            # Lo·∫°i b·ªè duplicate, gi·ªØ l·∫°i b·∫£n ghi cu·ªëi c√πng (m·ªõi nh·∫•t)
            deduplicated_df = combined_df.drop_duplicates(subset=[dedup_column], keep='last')

            total_after = len(deduplicated_df)
            duplicates_removed = total_before - total_after

            logger.info(f"üìä S·ªë b·∫£n ghi sau deduplicate: {total_after}")
            logger.info(f"üóëÔ∏è ƒê√£ lo·∫°i b·ªè {duplicates_removed} b·∫£n ghi tr√πng l·∫∑p")

            return deduplicated_df

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi ƒë·ªçc v√† deduplicate d·ªØ li·ªáu: {e}")
            raise

    def create_table_from_dataframe(self, df: pd.DataFrame, table_name: str):
        """
        T·∫°o b·∫£ng PostgreSQL t·ª´ DataFrame

        Args:
            df: DataFrame ch·ª©a d·ªØ li·ªáu
            table_name: T√™n b·∫£ng PostgreSQL
        """
        try:
            if df.empty:
                logger.warning("‚ö†Ô∏è DataFrame tr·ªëng, kh√¥ng th·ªÉ t·∫°o b·∫£ng")
                return

            # Mapping ki·ªÉu d·ªØ li·ªáu t·ª´ pandas sang PostgreSQL
            type_mapping = {
                'object': 'TEXT',
                'int64': 'BIGINT',
                'float64': 'DOUBLE PRECISION',
                'bool': 'BOOLEAN',
                'datetime64[ns]': 'TIMESTAMP'
            }

            # Sanitize column names for PostgreSQL
            import unicodedata
            import re

            def sanitize_column_name(name):
                # Remove accents and special characters
                name = unicodedata.normalize('NFD', str(name))
                name = name.encode('ascii', 'ignore').decode('ascii')
                # Replace spaces and special chars with underscore
                name = re.sub(r'[^a-zA-Z0-9]', '_', name)
                # Remove multiple underscores
                name = re.sub(r'_+', '_', name)
                # Remove leading/trailing underscores
                name = name.strip('_')
                # Ensure not empty
                if not name:
                    name = 'column'
                return name.lower()

            # T·∫°o c√¢u l·ªánh CREATE TABLE
            columns = []
            column_mapping = {}  # Map original name to sanitized name
            for col, dtype in df.dtypes.items():
                sanitized_col = sanitize_column_name(col)
                pg_type = type_mapping.get(str(dtype), 'TEXT')
                columns.append(f'"{sanitized_col}" {pg_type}')
                column_mapping[col] = sanitized_col

            # Th√™m primary key constraint n·∫øu c√≥ propertydetails_propertyid
            if 'propertydetails_propertyid' in [sanitize_column_name(col) for col in df.columns]:
                pk_column = sanitize_column_name('propertydetails_propertyid')
                columns = [col if not col.startswith(f'"{pk_column}"') else f'{col} PRIMARY KEY' for col in columns]

            create_table_sql = f"""
            DROP TABLE IF EXISTS {table_name};
            CREATE TABLE {table_name} (
                {', '.join(columns)}
            );
            """

            # Th·ª±c thi CREATE TABLE
            with self.engine.connect() as conn:
                conn.execute(text(create_table_sql))
                conn.commit()

            logger.info(f"‚úÖ ƒê√£ t·∫°o b·∫£ng {table_name} v·ªõi {len(columns)} c·ªôt")

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o b·∫£ng t·ª´ DataFrame: {e}")
            raise

    def insert_dataframe_to_postgres(self, df: pd.DataFrame, table_name: str, batch_size: int = 1000):
        """
        Insert DataFrame v√†o b·∫£ng PostgreSQL theo batch

        Args:
            df: DataFrame ch·ª©a d·ªØ li·ªáu
            table_name: T√™n b·∫£ng PostgreSQL
            batch_size: K√≠ch th∆∞·ªõc batch
        """
        try:
            if df.empty:
                logger.warning("‚ö†Ô∏è DataFrame tr·ªëng, kh√¥ng c√≥ g√¨ ƒë·ªÉ insert")
                return

            # Sanitize column names
            import unicodedata
            import re

            def sanitize_column_name(name):
                name = unicodedata.normalize('NFD', str(name))
                name = name.encode('ascii', 'ignore').decode('ascii')
                name = re.sub(r'[^a-zA-Z0-9]', '_', name)
                name = re.sub(r'_+', '_', name)
                name = name.strip('_')
                if not name:
                    name = 'column'
                return name.lower()

            # T·∫°o column mapping
            column_mapping = {col: sanitize_column_name(col) for col in df.columns}

            # Rename columns
            df_renamed = df.rename(columns=column_mapping)

            # Insert theo batch ƒë·ªÉ tr√°nh memory issues
            total_rows = len(df_renamed)
            logger.info(f"üì• B·∫Øt ƒë·∫ßu insert {total_rows} b·∫£n ghi v√†o {table_name}")

            for i in range(0, total_rows, batch_size):
                batch_df = df_renamed.iloc[i:i+batch_size]
                batch_df.to_sql(table_name, self.engine, if_exists='append', index=False)
                logger.info(f"üì• ƒê√£ insert batch {i//batch_size + 1}: {len(batch_df)} b·∫£n ghi (t·ªïng: {min(i+batch_size, total_rows)}/{total_rows})")

            logger.info(f"‚úÖ Ho√†n th√†nh insert {total_rows} b·∫£n ghi v√†o {table_name}")

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi insert DataFrame v√†o PostgreSQL: {e}")
            raise

    def get_parquet_files(self, bucket_name: str, prefix: str) -> List[str]:
        """L·∫•y danh s√°ch file Parquet t·ª´ MinIO s·ª≠ d·ª•ng boto3"""
        try:
            # S·ª≠ d·ª•ng boto3 ƒë·ªÉ k·∫øt n·ªëi MinIO
            s3_client = boto3.client(
                's3',
                endpoint_url=self.minio_config['endpoint'],
                aws_access_key_id=self.minio_config['access_key'],
                aws_secret_access_key=self.minio_config['secret_key'],
                region_name=self.minio_config.get('region', 'us-east-1')
            )

            # List objects v·ªõi prefix
            response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
            
            files = []
            if 'Contents' in response:
                for obj in response['Contents']:
                    key = obj['Key']
                    if key.endswith('.parquet'):
                        files.append(f"s3://{bucket_name}/{key}")
            
            logger.info(f"üìÅ T√¨m th·∫•y {len(files)} file Parquet")
            for file in files[:5]:  # Log first 5 files
                logger.info(f"  - {file}")
            return files
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l·∫•y danh s√°ch file v·ªõi boto3: {e}")
            return []

    def export_data(self, parquet_files: List[str], table_name: str, search_criteria: Dict[str, Any] = None):
        """Export t·∫•t c·∫£ d·ªØ li·ªáu t·ª´ Parquet files sang PostgreSQL"""
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu export {len(parquet_files)} file Parquet sang {table_name}")

        # Truncate b·∫£ng n·∫øu ƒë√£ t·ªìn t·∫°i ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu m·ªõi
        try:
            with self.engine.connect() as conn:
                # Ki·ªÉm tra b·∫£ng c√≥ t·ªìn t·∫°i kh√¥ng
                result = conn.execute(text(f"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = '{table_name}')"))
                table_exists = result.fetchone()[0]

                if table_exists:
                    logger.info(f"üóëÔ∏è Truncate b·∫£ng {table_name} ƒë·ªÉ l√†m m·ªõi d·ªØ li·ªáu")
                    conn.execute(text(f"TRUNCATE TABLE {table_name}"))
                    conn.commit()
                else:
                    logger.info(f"üìã B·∫£ng {table_name} ch∆∞a t·ªìn t·∫°i, s·∫Ω t·∫°o m·ªõi")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ truncate b·∫£ng: {e}")

        # ƒê·ªçc t·∫•t c·∫£ d·ªØ li·ªáu t·ª´ Delta Lake v√† deduplicate
        logger.info("üìä ƒê·ªçc v√† deduplicate d·ªØ li·ªáu t·ª´ Delta Lake...")
        all_data_df = self.read_and_deduplicate_delta_data(parquet_files)

        if all_data_df.empty:
            logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ export")
            return

        # Filter theo search criteria n·∫øu c√≥
        if search_criteria and search_criteria.get('city'):
            city_filter = search_criteria.get('city')
            logger.info(f"üîç Filter d·ªØ li·ªáu theo city: {city_filter}")

            # Filter d·ª±a tr√™n c·ªôt search_city
            if 'search_city' in all_data_df.columns:
                filtered_df = all_data_df[all_data_df['search_city'] == city_filter]
                logger.info(f"üìä Sau khi filter: {len(filtered_df)}/{len(all_data_df)} b·∫£n ghi cho city '{city_filter}'")
                all_data_df = filtered_df
            else:
                logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt 'search_city' ƒë·ªÉ filter")

        logger.info(f"üìä T·ªïng s·ªë b·∫£n ghi sau filter: {len(all_data_df)}")

        # T·∫°o b·∫£ng t·ª´ d·ªØ li·ªáu ƒë√£ filter
        self.create_table_from_dataframe(all_data_df, table_name)

        # Insert d·ªØ li·ªáu ƒë√£ filter
        self.insert_dataframe_to_postgres(all_data_df, table_name)

        logger.info("‚úÖ Ho√†n th√†nh export d·ªØ li·ªáu")

    def create_indexes(self, table_name: str):
        """T·∫°o indexes cho b·∫£ng ƒë·ªÉ t·ªëi ∆∞u performance"""
        # L·∫•y danh s√°ch c·ªôt th·ª±c t·∫ø t·ª´ b·∫£ng
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(f"SELECT column_name FROM information_schema.columns WHERE table_name = '{table_name}' ORDER BY column_name"))
                existing_columns = [row[0] for row in result.fetchall()]

            logger.info(f"üìã C√°c c·ªôt trong b·∫£ng {table_name}: {existing_columns}")

            # Mapping c√°c c·ªôt c√≥ th·ªÉ c√≥ index (lo·∫°i b·ªè propertydetails_propertyid v√¨ ƒë√£ l√† PRIMARY KEY)
            possible_indexes = {
                'ia_chi': 'idx_{table_name}_ia_chi',
                'url': 'idx_{table_name}_url',
                'latitude': 'idx_{table_name}_latitude',
                'longitude': 'idx_{table_name}_longitude',
                'city': 'idx_{table_name}_city',
                'propertytype': 'idx_{table_name}_propertytype',
                'ngay_ang': 'idx_{table_name}_ngay_ang'
            }

            indexes_to_create = []
            for col, index_name in possible_indexes.items():
                if col in existing_columns:
                    indexes_to_create.append(f"CREATE INDEX IF NOT EXISTS {index_name.format(table_name=table_name)} ON {table_name}({col})")

            if not indexes_to_create:
                logger.info("‚ö†Ô∏è Kh√¥ng c√≥ c·ªôt n√†o ph√π h·ª£p ƒë·ªÉ t·∫°o index")
                return

            # T·∫°o indexes
            with self.engine.connect() as conn:
                for index_sql in indexes_to_create:
                    try:
                        conn.execute(text(index_sql))
                        conn.commit()
                        logger.info(f"‚úÖ ƒê√£ t·∫°o index: {index_sql.split(' ON ')[1].split('(')[0]}")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o index cho c·ªôt {index_sql}: {e}")

            logger.info(f"‚úÖ ƒê√£ t·∫°o {len(indexes_to_create)} indexes")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o indexes: {e}")

    def verify_export(self, table_name: str, expected_count: int):
        """Verify s·ªë l∆∞·ª£ng d·ªØ li·ªáu ƒë√£ export"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                actual_count = result.fetchone()[0]

            logger.info(f"üìä S·ªë d√≤ng trong PostgreSQL: {actual_count}")
            logger.info(f"üìä S·ªë d√≤ng expected: {expected_count}")

            if actual_count == expected_count:
                logger.info("‚úÖ S·ªë l∆∞·ª£ng d·ªØ li·ªáu kh·ªõp!")
            else:
                logger.warning(f"‚ö†Ô∏è S·ªë l∆∞·ª£ng kh√¥ng kh·ªõp: {actual_count} vs {expected_count}")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi verify: {e}")

    def generate_and_run_queries(self, table_name: str, run_queries: bool = True, limit_results: int = 10) -> Dict[str, Any]:
        """Generate v√† ch·∫°y c√°c SQL queries analytics"""
        sql_gen = RealEstateSQLGenerator()
        results = {}

        queries = {
            'basic_stats': sql_gen.generate_basic_stats_query(table_name),
            'price_by_city': sql_gen.generate_price_by_city_query(table_name),
            'location_distribution': sql_gen.generate_property_type_distribution_query(table_name),
            'price_ranges': sql_gen.generate_price_ranges_query(table_name)
        }

        if run_queries:
            try:
                for query_name, query_sql in queries.items():
                    logger.info(f" Ch·∫°y query: {query_name}")
                    with self.engine.connect() as conn:
                        result = conn.execute(text(query_sql))
                        df = pd.DataFrame(result.fetchall(), columns=result.keys())
                        results[query_name] = df.head(limit_results)
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi ch·∫°y queries: {e}")

        return results

    def print_query_results(self, results: Dict[str, Any]):
        """In k·∫øt qu·∫£ queries"""
        for query_name, df in results.items():
            print(f"\n=== {query_name.upper()} ===")
            print(df.to_string(index=False))

    def plot_query_results(self, results: Dict[str, Any], save_path: Optional[str] = None):
        """V·∫Ω bi·ªÉu ƒë·ªì t·ª´ k·∫øt qu·∫£ queries"""
        if not PLOTTING_AVAILABLE:
            print("‚ö†Ô∏è Matplotlib kh√¥ng kh·∫£ d·ª•ng, kh√¥ng th·ªÉ v·∫Ω bi·ªÉu ƒë·ªì")
            return

        print("\nüìä ƒêang t·∫°o bi·ªÉu ƒë·ªì...")

        # T·∫°o figure v·ªõi subplots
        fig = plt.figure(figsize=(20, 16))
        fig.suptitle('Ph√¢n t√≠ch b·∫•t ƒë·ªông s·∫£n', fontsize=16, fontweight='bold')

        plot_count = 0
        max_plots = 6

        # 1. Basic stats
        if 'basic_stats' in results and not results['basic_stats'].empty and plot_count < max_plots:
            plot_count += 1
            plt.subplot(2, 3, plot_count)
            basic_stats = results['basic_stats'].iloc[0]

            labels = ['T·ªïng BƒêS', 'Gi√° TB\n(t·ª∑)', 'Di·ªán t√≠ch TB\n(m¬≤)', 'S·ªë TP']
            try:
                avg_price_str = str(basic_stats['avg_price'])
                avg_price_val = float(avg_price_str.replace(',', '')) / 1e9
            except:
                avg_price_val = 0

            values = [
                int(basic_stats['total_properties']),
                avg_price_val,
                float(basic_stats['avg_area']),
                int(basic_stats['cities_count'])
            ]

            bars = plt.bar(labels, values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
            plt.title('Th·ªëng k√™ c∆° b·∫£n', fontsize=12, fontweight='bold')
            plt.ylabel('Gi√° tr·ªã')
            plt.xticks(rotation=45, ha='right')

            # Th√™m gi√° tr·ªã tr√™n c·ªôt
            for bar, value in zip(bars, values):
                plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + bar.get_height()*0.01,
                        f'{value:.1f}', ha='center', va='bottom', fontsize=8)

        # 2. Price by city (Top 5)
        if 'price_by_city' in results and not results['price_by_city'].empty and plot_count < max_plots:
            plot_count += 1
            plt.subplot(2, 3, plot_count)
            price_city = results['price_by_city'].head(5)

            cities = [str(city)[:15] + '...' if len(str(city)) > 15 else str(city) for city in price_city['city']]

            prices = []
            for price in price_city['avg_price']:
                try:
                    price_str = str(price)
                    price_val = float(price_str.replace(',', '')) / 1e9
                    prices.append(price_val)
                except:
                    prices.append(0)

            plt.bar(range(len(cities)), prices, color='steelblue', alpha=0.8)
            plt.title('Gi√° TB theo th√†nh ph·ªë (Top 5)', fontsize=12, fontweight='bold')
            plt.xlabel('Th√†nh ph·ªë')
            plt.ylabel('Gi√° TB (t·ª∑ VNƒê)')
            plt.xticks(range(len(cities)), cities, rotation=45, ha='right', fontsize=8)

        # 3. Price ranges
        if 'price_ranges' in results and not results['price_ranges'].empty and plot_count < max_plots:
            plot_count += 1
            plt.subplot(2, 3, plot_count)
            price_ranges = results['price_ranges']

            plt.pie(price_ranges['percentage'], labels=price_ranges['price_range'],
                   autopct='%1.1f%%', startangle=90)
            plt.title('Ph√¢n b·ªë theo kho·∫£ng gi√°', fontsize=12, fontweight='bold')
            plt.axis('equal')

        # 4. Location distribution (Top 5)
        if 'location_distribution' in results and not results['location_distribution'].empty and plot_count < max_plots:
            plot_count += 1
            plt.subplot(2, 3, plot_count)
            location_dist = results['location_distribution'].head(5)

            locations = [loc[:20] + '...' if len(loc) > 20 else loc for loc in location_dist['location']]
            counts = location_dist['count']

            plt.barh(range(len(locations)), counts, color='teal', alpha=0.7)
            plt.title('Ph√¢n b·ªë theo khu v·ª±c (Top 5)', fontsize=12, fontweight='bold')
            plt.xlabel('S·ªë l∆∞·ª£ng')
            plt.yticks(range(len(locations)), locations, fontsize=8)

        # 5. Recent trends (if available)
        if 'recent_trends' in results and not results['recent_trends'].empty and plot_count < max_plots:
            plot_count += 1
            plt.subplot(2, 3, plot_count)
            trends = results['recent_trends'].copy()
            try:
                trends['date'] = pd.to_datetime(trends['date'])
                trends = trends.sort_values('date').tail(10)  # Last 10 days

                dates = trends['date'].dt.strftime('%m-%d')
                prices = []
                for price in trends['avg_price']:
                    try:
                        price_str = str(price)
                        price_val = float(price_str.replace(',', '')) / 1e9
                        prices.append(price_val)
                    except:
                        prices.append(0)

                plt.plot(range(len(dates)), prices, marker='o', linewidth=2, color='darkgreen')
                plt.title('Xu h∆∞·ªõng gi√° (10 ng√†y g·∫ßn nh·∫•t)', fontsize=12, fontweight='bold')
                plt.xlabel('Ng√†y')
                plt.ylabel('Gi√° TB (t·ª∑ VNƒê)')
                plt.xticks(range(len(dates)), dates, rotation=45, ha='right', fontsize=8)
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ v·∫Ω xu h∆∞·ªõng th·ªùi gian: {e}")

        # 6. Area distribution
        if 'location_distribution' in results and not results['location_distribution'].empty and plot_count < max_plots:
            plot_count += 1
            plt.subplot(2, 3, plot_count)
            location_dist = results['location_distribution'].head(5)

            locations = [loc[:20] + '...' if len(loc) > 20 else loc for loc in location_dist['location']]
            areas = location_dist['avg_area']

            plt.bar(range(len(locations)), areas, color='darkorange', alpha=0.8)
            plt.title('Di·ªán t√≠ch TB theo khu v·ª±c', fontsize=12, fontweight='bold')
            plt.xlabel('Khu v·ª±c')
            plt.ylabel('Di·ªán t√≠ch TB (m¬≤)')
            plt.xticks(range(len(locations)), locations, rotation=45, ha='right', fontsize=8)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"üíæ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì v√†o: {save_path}")
        else:
            plt.show()

        print("‚úÖ Ho√†n th√†nh v·∫Ω bi·ªÉu ƒë·ªì")

def main():
    parser = argparse.ArgumentParser(description='Ch·∫°y analytics queries tr√™n d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n trong PostgreSQL')
    parser.add_argument('--limit-results', type=int, default=10, help='Gi·ªõi h·∫°n s·ªë d√≤ng k·∫øt qu·∫£ queries')
    parser.add_argument('--plot', action='store_true', help='T·∫°o bi·ªÉu ƒë·ªì t·ª´ k·∫øt qu·∫£ analytics')
    parser.add_argument('--save-plot', type=str, help='L∆∞u bi·ªÉu ƒë·ªì v√†o file (v√≠ d·ª•: charts.png)')

    args = parser.parse_args()

    # C·∫•u h√¨nh PostgreSQL m·∫∑c ƒë·ªãnh
    POSTGRES_CONFIG = {}

    # T√¨m file credentials
    possible_paths = [
        'postgres_credentials.yaml',
        os.path.join(os.path.dirname(__file__), 'postgres_credentials.yaml')
    ]

    creds_file = None
    for path in possible_paths:
        if os.path.exists(path):
            creds_file = path
            break

    if creds_file:
        try:
            with open(creds_file, 'r', encoding='utf-8') as f:
                creds = yaml.safe_load(f)
            POSTGRES_CONFIG['host'] = creds.get('postgresql', {}).get('host')
            POSTGRES_CONFIG['port'] = creds.get('postgresql', {}).get('port')
            POSTGRES_CONFIG['database'] = creds.get('postgresql', {}).get('database')
            POSTGRES_CONFIG['user'] = creds.get('postgresql', {}).get('user')
            POSTGRES_CONFIG['password'] = creds.get('postgresql', {}).get('password')
            logger.info(f"ƒê√£ t·∫£i credentials t·ª´: {creds_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói khi t·∫£i credentials: {e}")

    # Fallback defaults
    if not POSTGRES_CONFIG.get('host'):
        POSTGRES_CONFIG['host'] = 'localhost'
    if not POSTGRES_CONFIG.get('port'):
        POSTGRES_CONFIG['port'] = 5432
    if not POSTGRES_CONFIG.get('database'):
        POSTGRES_CONFIG['database'] = 'real_estate_db'
    if not POSTGRES_CONFIG.get('user'):
        POSTGRES_CONFIG['user'] = 'postgres'
    if not POSTGRES_CONFIG.get('password'):
        POSTGRES_CONFIG['password'] = 'postgres123'

    # C·∫•u h√¨nh MinIO
    MINIO_CONFIG = {
        'endpoint': 'http://127.0.0.1:9000',
        'access_key': 'minioadmin',
        'secret_key': 'minioadmin',
        'region': 'us-east-1',
        'use_ssl': False
    }

    # T·∫°o PostgreSQL URL
    postgres_url = f"postgresql://{POSTGRES_CONFIG['user']}:{POSTGRES_CONFIG['password']}@{POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}/{POSTGRES_CONFIG['database']}"

    # Kh·ªüi t·∫°o exporter
    exporter = MinioToPostgresExporter(postgres_url, MINIO_CONFIG)

    try:
        # K·∫øt n·ªëi PostgreSQL
        exporter.connect_postgres()

        # Ch·ªâ ch·∫°y queries tr√™n d·ªØ li·ªáu ƒë√£ c√≥
        table_name = "real_estate_properties"
        logger.info("Generating v√† ch·∫°y c√°c c√¢u l·ªánh SQL truy v·∫•n...")
        query_results = exporter.generate_and_run_queries(
            table_name=table_name,
            run_queries=True,
            limit_results=args.limit_results
        )
        exporter.print_query_results(query_results)

        # V·∫Ω bi·ªÉu ƒë·ªì n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if args.plot or args.save_plot:
            exporter.plot_query_results(query_results, args.save_plot)

        logger.info(" Ho√†n th√†nh generate queries!")

    except Exception as e:
        logger.error(f" L·ªói trong qu√° tr√¨nh ch·∫°y queries: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()