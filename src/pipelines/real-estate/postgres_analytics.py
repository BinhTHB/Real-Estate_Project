#!/usr/bin/env python3
"""
PostgreSQL Analytics Runner - Ch·∫°y analytics queries tr√™n d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n
"""

import os
import sys
import logging
import pandas as pd
import duckdb
import boto3
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError
import yaml
from typing import Optional, List, Dict, Any
from sql_query_generator import RealEstateSQLGenerator
import argparse

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MinioToPostgresExporter:
    """Class ƒë·ªÉ ch·∫°y analytics queries tr√™n d·ªØ li·ªáu PostgreSQL"""

    def __init__(self, postgres_url: str, minio_config: Dict[str, Any]):
        """
        Kh·ªüi t·∫°o exporter

        Args:
            postgres_url: PostgreSQL connection URL
            minio_config: C·∫•u h√¨nh MinIO (endpoint, credentials, etc.)
        """
        self.postgres_url = postgres_url
        self.minio_config = minio_config
        self.engine = None
        self.duckdb_conn = None

    def connect_postgres(self):
        """K·∫øt n·ªëi t·ªõi PostgreSQL"""
        try:
            self.engine = create_engine(
                self.postgres_url,
                pool_size=10,
                max_overflow=20,
                pool_timeout=30,
                pool_recycle=3600
            )
            # Test connection
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            logger.info("‚úÖ K·∫øt n·ªëi PostgreSQL th√†nh c√¥ng")
        except SQLAlchemyError as e:
            logger.error(f"‚ùå L·ªói k·∫øt n·ªëi PostgreSQL: {e}")
            raise

    def connect_minio_duckdb(self):
        """K·∫øt n·ªëi t·ªõi MinIO qua DuckDB"""
        try:
            # C·∫•u h√¨nh DuckDB ƒë·ªÉ k·∫øt n·ªëi MinIO S3
            self.duckdb_conn = duckdb.connect()
            self.duckdb_conn.execute(f"""
                INSTALL httpfs;
                LOAD httpfs;
                SET s3_endpoint='{self.minio_config['endpoint']}';
                SET s3_access_key_id='{self.minio_config['access_key']}';
                SET s3_secret_access_key='{self.minio_config['secret_key']}';
                SET s3_use_ssl=false;
                SET s3_url_style='path';
            """)
            logger.info("‚úÖ K·∫øt n·ªëi MinIO qua DuckDB th√†nh c√¥ng")
        except Exception as e:
            logger.error(f"‚ùå L·ªói k·∫øt n·ªëi MinIO: {e}")
            raise

    def create_table_from_parquet(self, file_url: str, table_name: str, batch_size: int = 1000, create_table_only: bool = False):
        """
        T·∫°o b·∫£ng PostgreSQL t·ª´ file Parquet v√† insert d·ªØ li·ªáu

        Args:
            file_url: URL c·ªßa file Parquet trong MinIO
            table_name: T√™n b·∫£ng PostgreSQL
            batch_size: K√≠ch th∆∞·ªõc batch cho insert
            create_table_only: Ch·ªâ t·∫°o b·∫£ng, kh√¥ng insert d·ªØ li·ªáu
        """
        try:
            # Convert s3:// URL to http:// URL for DuckDB
            if file_url.startswith('s3://'):
                # s3://bucket/key -> http://endpoint/bucket/key
                bucket_and_key = file_url[5:]  # Remove 's3://'
                http_url = f"{self.minio_config['endpoint']}/{bucket_and_key}"
            else:
                http_url = file_url

            logger.info(f"üìÑ ƒê·ªçc file t·ª´: {http_url}")

            # ƒê·ªçc sample ƒë·ªÉ x√°c ƒë·ªãnh schema
            df_sample = self.duckdb_conn.sql(f"SELECT * FROM read_parquet('{http_url}') LIMIT 1").df()

            if df_sample.empty:
                logger.warning(f"‚ö†Ô∏è File {http_url} tr·ªëng, b·ªè qua")
                return

            # Mapping ki·ªÉu d·ªØ li·ªáu t·ª´ pandas sang PostgreSQL
            type_mapping = {
                'object': 'TEXT',
                'int64': 'BIGINT',
                'float64': 'DOUBLE PRECISION',
                'bool': 'BOOLEAN',
                'datetime64[ns]': 'TIMESTAMP'
            }

            # Sanitize column names for PostgreSQL
            import unicodedata
            import re
            
            def sanitize_column_name(name):
                # Remove accents and special characters
                name = unicodedata.normalize('NFD', str(name))
                name = name.encode('ascii', 'ignore').decode('ascii')
                # Replace spaces and special chars with underscore
                name = re.sub(r'[^a-zA-Z0-9]', '_', name)
                # Remove multiple underscores
                name = re.sub(r'_+', '_', name)
                # Remove leading/trailing underscores
                name = name.strip('_')
                # Ensure not empty
                if not name:
                    name = 'column'
                return name.lower()

            # T·∫°o c√¢u l·ªánh CREATE TABLE
            columns = []
            column_mapping = {}  # Map original name to sanitized name
            for col, dtype in df_sample.dtypes.items():
                sanitized_col = sanitize_column_name(col)
                pg_type = type_mapping.get(str(dtype), 'TEXT')
                columns.append(f'"{sanitized_col}" {pg_type}')
                column_mapping[col] = sanitized_col

            create_table_sql = f"""
            DROP TABLE IF EXISTS {table_name};
            CREATE TABLE {table_name} (
                {', '.join(columns)}
            );
            """

            # Th·ª±c thi CREATE TABLE
            with self.engine.connect() as conn:
                conn.execute(text(create_table_sql))
                conn.commit()

            logger.info(f"‚úÖ ƒê√£ t·∫°o b·∫£ng {table_name}")

            # Ch·ªâ insert d·ªØ li·ªáu n·∫øu kh√¥ng ph·∫£i ch·∫ø ƒë·ªô create_table_only
            if not create_table_only:
                # Insert d·ªØ li·ªáu theo batch
                offset = 0
                while True:
                    batch_df = self.duckdb_conn.sql(f"SELECT * FROM read_parquet('{http_url}') LIMIT {batch_size} OFFSET {offset}").df()
                    if batch_df.empty:
                        break

                    # Rename columns to sanitized names
                    batch_df_renamed = batch_df.rename(columns=column_mapping)

                    # Insert batch
                    batch_df_renamed.to_sql(table_name, self.engine, if_exists='append', index=False)
                    offset += batch_size
                    logger.info(f"üì• ƒê√£ insert {len(batch_df)} d√≤ng (offset: {offset})")

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o b·∫£ng t·ª´ {file_url}: {e}")
            raise

    def insert_data_from_parquet(self, file_url: str, table_name: str, batch_size: int = 1000):
        """
        Insert d·ªØ li·ªáu t·ª´ file Parquet v√†o b·∫£ng PostgreSQL ƒë√£ t·ªìn t·∫°i

        Args:
            file_url: URL c·ªßa file Parquet trong MinIO
            table_name: T√™n b·∫£ng PostgreSQL
            batch_size: K√≠ch th∆∞·ªõc batch cho insert
        """
        try:
            # Convert s3:// URL to http:// URL for DuckDB
            if file_url.startswith('s3://'):
                bucket_and_key = file_url[5:]  # Remove 's3://'
                http_url = f"{self.minio_config['endpoint']}/{bucket_and_key}"
            else:
                http_url = file_url

            logger.info(f"üìÑ Insert d·ªØ li·ªáu t·ª´: {http_url}")

            # ƒê·ªçc sample ƒë·ªÉ l·∫•y column mapping (gi·∫£ s·ª≠ b·∫£ng ƒë√£ ƒë∆∞·ª£c t·∫°o v·ªõi c√πng schema)
            df_sample = self.duckdb_conn.sql(f"SELECT * FROM read_parquet('{http_url}') LIMIT 1").df()

            if df_sample.empty:
                logger.warning(f"‚ö†Ô∏è File {http_url} tr·ªëng, b·ªè qua")
                return

            # Sanitize column names (gi·ªëng nh∆∞ trong create_table_from_parquet)
            import unicodedata
            import re

            def sanitize_column_name(name):
                name = unicodedata.normalize('NFD', str(name))
                name = name.encode('ascii', 'ignore').decode('ascii')
                name = re.sub(r'[^a-zA-Z0-9]', '_', name)
                name = re.sub(r'_+', '_', name)
                name = name.strip('_')
                if not name:
                    name = 'column'
                return name.lower()

            # T·∫°o column mapping
            column_mapping = {}
            for col in df_sample.columns:
                column_mapping[col] = sanitize_column_name(col)

            # Insert d·ªØ li·ªáu theo batch
            offset = 0
            total_inserted = 0
            while True:
                batch_df = self.duckdb_conn.sql(f"SELECT * FROM read_parquet('{http_url}') LIMIT {batch_size} OFFSET {offset}").df()
                if batch_df.empty:
                    break

                # Rename columns to sanitized names
                batch_df_renamed = batch_df.rename(columns=column_mapping)

                # Insert batch
                batch_df_renamed.to_sql(table_name, self.engine, if_exists='append', index=False)
                batch_count = len(batch_df)
                total_inserted += batch_count
                offset += batch_size
                logger.info(f"üì• ƒê√£ insert {batch_count} d√≤ng (t·ªïng: {total_inserted}, offset: {offset})")

            logger.info(f"‚úÖ Ho√†n th√†nh insert {total_inserted} d√≤ng t·ª´ {file_url}")

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi insert d·ªØ li·ªáu t·ª´ {file_url}: {e}")
            raise

    def read_and_deduplicate_delta_data(self, parquet_files: List[str]) -> pd.DataFrame:
        """
        ƒê·ªçc t·∫•t c·∫£ d·ªØ li·ªáu t·ª´ Delta Lake v√† lo·∫°i b·ªè duplicate d·ª±a tr√™n propertydetails_propertyid

        Args:
            parquet_files: Danh s√°ch file Parquet

        Returns:
            DataFrame ƒë√£ deduplicate
        """
        try:
            all_dataframes = []

            for file_url in parquet_files:
                # Convert s3:// URL to http:// URL for DuckDB
                if file_url.startswith('s3://'):
                    bucket_and_key = file_url[5:]  # Remove 's3://'
                    http_url = f"{self.minio_config['endpoint']}/{bucket_and_key}"
                else:
                    http_url = file_url

                logger.info(f"üìÑ ƒê·ªçc file: {http_url}")

                # ƒê·ªçc to√†n b·ªô file
                df = self.duckdb_conn.sql(f"SELECT * FROM read_parquet('{http_url}')").df()
                if not df.empty:
                    all_dataframes.append(df)

            if not all_dataframes:
                logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu t·ª´ c√°c file Parquet")
                return pd.DataFrame()

            # G·ªôp t·∫•t c·∫£ DataFrames
            combined_df = pd.concat(all_dataframes, ignore_index=True)

            # ƒê·∫øm s·ªë b·∫£n ghi tr∆∞·ªõc deduplicate
            total_before = len(combined_df)
            logger.info(f"üìä T·ªïng s·ªë b·∫£n ghi tr∆∞·ªõc deduplicate: {total_before}")

            # Deduplicate d·ª±a tr√™n propertydetails_propertyid (n·∫øu c√≥) ho·∫∑c url
            if 'propertydetails_propertyid' in combined_df.columns:
                dedup_column = 'propertydetails_propertyid'
            elif 'url' in combined_df.columns:
                dedup_column = 'url'
            else:
                logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt ƒë·ªÉ deduplicate, gi·ªØ nguy√™n d·ªØ li·ªáu")
                return combined_df

            # Lo·∫°i b·ªè duplicate, gi·ªØ l·∫°i b·∫£n ghi cu·ªëi c√πng (m·ªõi nh·∫•t)
            deduplicated_df = combined_df.drop_duplicates(subset=[dedup_column], keep='last')

            total_after = len(deduplicated_df)
            duplicates_removed = total_before - total_after

            logger.info(f"üìä S·ªë b·∫£n ghi sau deduplicate: {total_after}")
            logger.info(f"üóëÔ∏è ƒê√£ lo·∫°i b·ªè {duplicates_removed} b·∫£n ghi tr√πng l·∫∑p")

            return deduplicated_df

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi ƒë·ªçc v√† deduplicate d·ªØ li·ªáu: {e}")
            raise

    def create_table_from_dataframe(self, df: pd.DataFrame, table_name: str):
        """
        T·∫°o b·∫£ng PostgreSQL t·ª´ DataFrame

        Args:
            df: DataFrame ch·ª©a d·ªØ li·ªáu
            table_name: T√™n b·∫£ng PostgreSQL
        """
        try:
            if df.empty:
                logger.warning("‚ö†Ô∏è DataFrame tr·ªëng, kh√¥ng th·ªÉ t·∫°o b·∫£ng")
                return

            # Mapping ki·ªÉu d·ªØ li·ªáu t·ª´ pandas sang PostgreSQL
            type_mapping = {
                'object': 'TEXT',
                'int64': 'BIGINT',
                'float64': 'DOUBLE PRECISION',
                'bool': 'BOOLEAN',
                'datetime64[ns]': 'TIMESTAMP'
            }

            # Sanitize column names for PostgreSQL
            import unicodedata
            import re

            def sanitize_column_name(name):
                # Remove accents and special characters
                name = unicodedata.normalize('NFD', str(name))
                name = name.encode('ascii', 'ignore').decode('ascii')
                # Replace spaces and special chars with underscore
                name = re.sub(r'[^a-zA-Z0-9]', '_', name)
                # Remove multiple underscores
                name = re.sub(r'_+', '_', name)
                # Remove leading/trailing underscores
                name = name.strip('_')
                # Ensure not empty
                if not name:
                    name = 'column'
                return name.lower()

            # T·∫°o c√¢u l·ªánh CREATE TABLE
            columns = []
            column_mapping = {}  # Map original name to sanitized name
            for col, dtype in df.dtypes.items():
                sanitized_col = sanitize_column_name(col)
                pg_type = type_mapping.get(str(dtype), 'TEXT')
                columns.append(f'"{sanitized_col}" {pg_type}')
                column_mapping[col] = sanitized_col

            # Th√™m primary key constraint n·∫øu c√≥ propertydetails_propertyid
            if 'propertydetails_propertyid' in [sanitize_column_name(col) for col in df.columns]:
                pk_column = sanitize_column_name('propertydetails_propertyid')
                columns = [col if not col.startswith(f'"{pk_column}"') else f'{col} PRIMARY KEY' for col in columns]

            create_table_sql = f"""
            DROP TABLE IF EXISTS {table_name};
            CREATE TABLE {table_name} (
                {', '.join(columns)}
            );
            """

            # Th·ª±c thi CREATE TABLE
            with self.engine.connect() as conn:
                conn.execute(text(create_table_sql))
                conn.commit()

            logger.info(f"‚úÖ ƒê√£ t·∫°o b·∫£ng {table_name} v·ªõi {len(columns)} c·ªôt")

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o b·∫£ng t·ª´ DataFrame: {e}")
            raise

    def insert_dataframe_to_postgres(self, df: pd.DataFrame, table_name: str, batch_size: int = 1000):
        """
        Insert DataFrame v√†o b·∫£ng PostgreSQL theo batch

        Args:
            df: DataFrame ch·ª©a d·ªØ li·ªáu
            table_name: T√™n b·∫£ng PostgreSQL
            batch_size: K√≠ch th∆∞·ªõc batch
        """
        try:
            if df.empty:
                logger.warning("‚ö†Ô∏è DataFrame tr·ªëng, kh√¥ng c√≥ g√¨ ƒë·ªÉ insert")
                return

            # Sanitize column names
            import unicodedata
            import re

            def sanitize_column_name(name):
                name = unicodedata.normalize('NFD', str(name))
                name = name.encode('ascii', 'ignore').decode('ascii')
                name = re.sub(r'[^a-zA-Z0-9]', '_', name)
                name = re.sub(r'_+', '_', name)
                name = name.strip('_')
                if not name:
                    name = 'column'
                return name.lower()

            # T·∫°o column mapping
            column_mapping = {col: sanitize_column_name(col) for col in df.columns}

            # Rename columns
            df_renamed = df.rename(columns=column_mapping)

            # Insert theo batch ƒë·ªÉ tr√°nh memory issues
            total_rows = len(df_renamed)
            logger.info(f"üì• B·∫Øt ƒë·∫ßu insert {total_rows} b·∫£n ghi v√†o {table_name}")

            for i in range(0, total_rows, batch_size):
                batch_df = df_renamed.iloc[i:i+batch_size]
                batch_df.to_sql(table_name, self.engine, if_exists='append', index=False)
                logger.info(f"üì• ƒê√£ insert batch {i//batch_size + 1}: {len(batch_df)} b·∫£n ghi (t·ªïng: {min(i+batch_size, total_rows)}/{total_rows})")

            logger.info(f"‚úÖ Ho√†n th√†nh insert {total_rows} b·∫£n ghi v√†o {table_name}")

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi insert DataFrame v√†o PostgreSQL: {e}")
            raise

    def get_parquet_files(self, bucket_name: str, prefix: str) -> List[str]:
        """L·∫•y danh s√°ch file Parquet t·ª´ MinIO s·ª≠ d·ª•ng boto3"""
        try:
            # S·ª≠ d·ª•ng boto3 ƒë·ªÉ k·∫øt n·ªëi MinIO
            s3_client = boto3.client(
                's3',
                endpoint_url=self.minio_config['endpoint'],
                aws_access_key_id=self.minio_config['access_key'],
                aws_secret_access_key=self.minio_config['secret_key'],
                region_name=self.minio_config.get('region', 'us-east-1')
            )

            # List objects v·ªõi prefix
            response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
            
            files = []
            if 'Contents' in response:
                for obj in response['Contents']:
                    key = obj['Key']
                    if key.endswith('.parquet'):
                        files.append(f"s3://{bucket_name}/{key}")
            
            logger.info(f"üìÅ T√¨m th·∫•y {len(files)} file Parquet")
            for file in files[:5]:  # Log first 5 files
                logger.info(f"  - {file}")
            return files
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l·∫•y danh s√°ch file v·ªõi boto3: {e}")
            return []

    def export_data(self, parquet_files: List[str], table_name: str):
        """Export t·∫•t c·∫£ d·ªØ li·ªáu t·ª´ Parquet files sang PostgreSQL"""
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu export {len(parquet_files)} file Parquet sang {table_name}")

        # Truncate b·∫£ng n·∫øu ƒë√£ t·ªìn t·∫°i ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu m·ªõi
        try:
            with self.engine.connect() as conn:
                # Ki·ªÉm tra b·∫£ng c√≥ t·ªìn t·∫°i kh√¥ng
                result = conn.execute(text(f"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = '{table_name}')"))
                table_exists = result.fetchone()[0]

                if table_exists:
                    logger.info(f"üóëÔ∏è Truncate b·∫£ng {table_name} ƒë·ªÉ l√†m m·ªõi d·ªØ li·ªáu")
                    conn.execute(text(f"TRUNCATE TABLE {table_name}"))
                    conn.commit()
                else:
                    logger.info(f"üìã B·∫£ng {table_name} ch∆∞a t·ªìn t·∫°i, s·∫Ω t·∫°o m·ªõi")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ truncate b·∫£ng: {e}")

        # ƒê·ªçc t·∫•t c·∫£ d·ªØ li·ªáu t·ª´ Delta Lake v√† deduplicate
        logger.info("üìä ƒê·ªçc v√† deduplicate d·ªØ li·ªáu t·ª´ Delta Lake...")
        all_data_df = self.read_and_deduplicate_delta_data(parquet_files)

        if all_data_df.empty:
            logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ export")
            return

        logger.info(f"üìä T·ªïng s·ªë b·∫£n ghi sau deduplicate: {len(all_data_df)}")

        # T·∫°o b·∫£ng t·ª´ d·ªØ li·ªáu ƒë√£ deduplicate
        self.create_table_from_dataframe(all_data_df, table_name)

        # Insert d·ªØ li·ªáu ƒë√£ deduplicate
        self.insert_dataframe_to_postgres(all_data_df, table_name)

        logger.info("‚úÖ Ho√†n th√†nh export d·ªØ li·ªáu")

    def create_indexes(self, table_name: str):
        """T·∫°o indexes cho b·∫£ng ƒë·ªÉ t·ªëi ∆∞u performance"""
        # L·∫•y danh s√°ch c·ªôt th·ª±c t·∫ø t·ª´ b·∫£ng
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(f"SELECT column_name FROM information_schema.columns WHERE table_name = '{table_name}' ORDER BY column_name"))
                existing_columns = [row[0] for row in result.fetchall()]

            logger.info(f"üìã C√°c c·ªôt trong b·∫£ng {table_name}: {existing_columns}")

            # Mapping c√°c c·ªôt c√≥ th·ªÉ c√≥ index (lo·∫°i b·ªè propertydetails_propertyid v√¨ ƒë√£ l√† PRIMARY KEY)
            possible_indexes = {
                'ia_chi': 'idx_{table_name}_ia_chi',
                'url': 'idx_{table_name}_url',
                'latitude': 'idx_{table_name}_latitude',
                'longitude': 'idx_{table_name}_longitude',
                'city': 'idx_{table_name}_city',
                'propertytype': 'idx_{table_name}_propertytype',
                'ngay_ang': 'idx_{table_name}_ngay_ang'
            }

            indexes_to_create = []
            for col, index_name in possible_indexes.items():
                if col in existing_columns:
                    indexes_to_create.append(f"CREATE INDEX IF NOT EXISTS {index_name.format(table_name=table_name)} ON {table_name}({col})")

            if not indexes_to_create:
                logger.info("‚ö†Ô∏è Kh√¥ng c√≥ c·ªôt n√†o ph√π h·ª£p ƒë·ªÉ t·∫°o index")
                return

            # T·∫°o indexes
            with self.engine.connect() as conn:
                for index_sql in indexes_to_create:
                    try:
                        conn.execute(text(index_sql))
                        conn.commit()
                        logger.info(f"‚úÖ ƒê√£ t·∫°o index: {index_sql.split(' ON ')[1].split('(')[0]}")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o index cho c·ªôt {index_sql}: {e}")

            logger.info(f"‚úÖ ƒê√£ t·∫°o {len(indexes_to_create)} indexes")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o indexes: {e}")

    def verify_export(self, table_name: str, expected_count: int):
        """Verify s·ªë l∆∞·ª£ng d·ªØ li·ªáu ƒë√£ export"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                actual_count = result.fetchone()[0]

            logger.info(f"üìä S·ªë d√≤ng trong PostgreSQL: {actual_count}")
            logger.info(f"üìä S·ªë d√≤ng expected: {expected_count}")

            if actual_count == expected_count:
                logger.info("‚úÖ S·ªë l∆∞·ª£ng d·ªØ li·ªáu kh·ªõp!")
            else:
                logger.warning(f"‚ö†Ô∏è S·ªë l∆∞·ª£ng kh√¥ng kh·ªõp: {actual_count} vs {expected_count}")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi verify: {e}")

    def generate_and_run_queries(self, table_name: str, run_queries: bool = True, limit_results: int = 10) -> Dict[str, Any]:
        """Generate v√† ch·∫°y c√°c SQL queries analytics"""
        sql_gen = RealEstateSQLGenerator()
        results = {}

        queries = {
            'basic_stats': sql_gen.generate_basic_stats_query(table_name),
            'price_by_city': sql_gen.generate_price_by_city_query(table_name),
            'location_distribution': sql_gen.generate_property_type_distribution_query(table_name),
            'price_ranges': sql_gen.generate_price_ranges_query(table_name)
        }

        if run_queries:
            try:
                for query_name, query_sql in queries.items():
                    logger.info(f"üîç Ch·∫°y query: {query_name}")
                    with self.engine.connect() as conn:
                        result = conn.execute(text(query_sql))
                        df = pd.DataFrame(result.fetchall(), columns=result.keys())
                        results[query_name] = df.head(limit_results)
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi ch·∫°y queries: {e}")

        return results

    def print_query_results(self, results: Dict[str, Any]):
        """In k·∫øt qu·∫£ queries"""
        for query_name, df in results.items():
            print(f"\n=== {query_name.upper()} ===")
            print(df.to_string(index=False))

def main():
    parser = argparse.ArgumentParser(description='Ch·∫°y analytics queries tr√™n d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n trong PostgreSQL')
    parser.add_argument('--limit-results', type=int, default=10, help='Gi·ªõi h·∫°n s·ªë d√≤ng k·∫øt qu·∫£ queries')

    args = parser.parse_args()

    # C·∫•u h√¨nh PostgreSQL m·∫∑c ƒë·ªãnh
    POSTGRES_CONFIG = {}

    # T√¨m file credentials
    possible_paths = [
        'postgres_credentials.yaml',
        os.path.join(os.path.dirname(__file__), 'postgres_credentials.yaml')
    ]

    creds_file = None
    for path in possible_paths:
        if os.path.exists(path):
            creds_file = path
            break

    if creds_file:
        try:
            with open(creds_file, 'r', encoding='utf-8') as f:
                creds = yaml.safe_load(f)
            POSTGRES_CONFIG['host'] = creds.get('postgresql', {}).get('host')
            POSTGRES_CONFIG['port'] = creds.get('postgresql', {}).get('port')
            POSTGRES_CONFIG['database'] = creds.get('postgresql', {}).get('database')
            POSTGRES_CONFIG['user'] = creds.get('postgresql', {}).get('user')
            POSTGRES_CONFIG['password'] = creds.get('postgresql', {}).get('password')
            logger.info(f"‚úÖ ƒê√£ t·∫£i credentials t·ª´: {creds_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói khi t·∫£i credentials: {e}")

    # Fallback defaults
    if not POSTGRES_CONFIG.get('host'):
        POSTGRES_CONFIG['host'] = 'localhost'
    if not POSTGRES_CONFIG.get('port'):
        POSTGRES_CONFIG['port'] = 5432
    if not POSTGRES_CONFIG.get('database'):
        POSTGRES_CONFIG['database'] = 'real_estate_db'
    if not POSTGRES_CONFIG.get('user'):
        POSTGRES_CONFIG['user'] = 'postgres'
    if not POSTGRES_CONFIG.get('password'):
        POSTGRES_CONFIG['password'] = 'postgres123'

    # C·∫•u h√¨nh MinIO
    MINIO_CONFIG = {
        'endpoint': 'http://127.0.0.1:9000',
        'access_key': 'minioadmin',
        'secret_key': 'minioadmin',
        'region': 'us-east-1',
        'use_ssl': False
    }

    # T·∫°o PostgreSQL URL
    postgres_url = f"postgresql://{POSTGRES_CONFIG['user']}:{POSTGRES_CONFIG['password']}@{POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}/{POSTGRES_CONFIG['database']}"

    # Kh·ªüi t·∫°o exporter
    exporter = MinioToPostgresExporter(postgres_url, MINIO_CONFIG)

    try:
        # K·∫øt n·ªëi PostgreSQL
        exporter.connect_postgres()

        # Ch·ªâ ch·∫°y queries tr√™n d·ªØ li·ªáu ƒë√£ c√≥
        table_name = "real_estate_properties"
        logger.info("üîç Generating v√† ch·∫°y c√°c c√¢u l·ªánh SQL truy v·∫•n...")
        query_results = exporter.generate_and_run_queries(
            table_name=table_name,
            run_queries=True,
            limit_results=args.limit_results
        )
        exporter.print_query_results(query_results)

        logger.info("üéâ Ho√†n th√†nh generate queries!")

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh ch·∫°y queries: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()